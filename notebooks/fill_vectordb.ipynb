{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to https://codeawake.com/blog/postgresql-vector-database\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# import pdfminer\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "from sqlalchemy import Text\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import src.setting as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pgai to setup necessary fucntions and tables in my vector DB, see https://github.com/timescale/pgai/tree/main/docs\n",
    "# import pgai\n",
    "# pgai.install(DB_URL)\n",
    "# All of the pgai objects are installed into the ai schema.\n",
    "\n",
    "# install pgau command line tool by runnign following command in the terminal: uv add pgai[vectorizer-worker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create vector DB with postgresql\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Vector(Base):\n",
    "    __tablename__ = \"postgres\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)\n",
    "    text: Mapped[str] = mapped_column(Text)\n",
    "    vector = mapped_column(\n",
    "        Vector(1024)\n",
    "    )  # set embedding dimensions, match with chosen embedding model\n",
    "    metadata_: Mapped[dict | None] = mapped_column(\"metadata\", JSONB)+\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Vector(id={self.id}, text={self.text[:50]}..., metadata={self.metadata_})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.asyncio import async_sessionmaker, create_async_engine\n",
    "\n",
    "DB_URL = \"postgresql+asyncpg://admin:postgres@localhost:5432/postgres\"\n",
    "\n",
    "engine = create_async_engine(DB_URL)\n",
    "\n",
    "\n",
    "async def db_create():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "    print(engine.url, \"connected and tables created.\")\n",
    "\n",
    "\n",
    "engine = create_async_engine(DB_URL)\n",
    "Session = async_sessionmaker(engine, expire_on_commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_source_name = \"Koks et al - 2022 - Brief communication\"\n",
    "\"../\" + s.settings.PATH_DATA + f\"text_sources/{text_source_name}.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract text from pdf with unstructured, good for RAG systems + document analysis\n",
    "from unstructured.partition.auto import partition\n",
    "import nltk  # unsupervised sentence tokenizer (https://www.nltk.org/api/nltk.tokenize.punkt.html)\n",
    "\n",
    "## load NLTK resource file for sentence tokenizer\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "\n",
    "text_source_name = \"Koks et al 2022 Brief communication\"\n",
    "\n",
    "blocks = partition(\n",
    "    filename=\"../\" + s.settings.PATH_DATA + f\"text_sources/{text_source_name}.pdf\"\n",
    ")\n",
    "for block in blocks:\n",
    "    print(f\"{block.category}: {block.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../\" + s.settings.PATH_DATA + f\"text_sources/{text_source_name}.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## extract text from pdf via pypdf\n",
    "# import pypdf\n",
    "# import json\n",
    "\n",
    "# def extract_text_from_pdf(file_path: str) -> str:\n",
    "#     text_list = []\n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         reader = pypdf.PdfReader(file)\n",
    "#         for page in reader.pages:\n",
    "#             text_list.append(page.extract_text())\n",
    "#             #text_list.append(page.extract_text() + \" \")\n",
    "#     return \"  \".join(text_list)\n",
    "\n",
    "\n",
    "# text_source_name = \"Koks et al - 2022 - Brief communication\"  # define which pdf should be read converted to txt\n",
    "# with open(\"../\" + s.PATH_DATA + f\"{text_source_name}.txt\", \"w+\") as f:\n",
    "#     json.dump(\n",
    "#         extract_text_from_pdf(\"../\" + s.PATH_DATA + f\"text_sources/{text_source_name}.pdf\"),\n",
    "#         f,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## extracting text from pdfs using pdfminer\n",
    "\n",
    "\n",
    "# docs = []\n",
    "# DOCS_DIR = \"../\" + s.settings.PATH_DATA + \"text_sources/\"\n",
    "\n",
    "# for filename in os.listdir(DOCS_DIR):\n",
    "#     if filename.endswith(\".pdf\"):\n",
    "#         file_path = os.path.join(DOCS_DIR, filename)\n",
    "#         text = extract_text(file_path)\n",
    "#         print(text)\n",
    "#         docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define recursive chunking, see, https://github.com/ruizguille/rag-from-scratch/blob/master/app/splitter.py\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "tiktoken_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sentence_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "\n",
    "def token_size(text):\n",
    "    return len(tiktoken_tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def split_by_separator(text: str, sep: str) -> list[str]:\n",
    "    splits = text.split(sep)\n",
    "    res = [s + sep for s in splits[:-1]]\n",
    "    if splits[-1]:\n",
    "        res.append(splits[-1])\n",
    "    return res\n",
    "\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    spans = [s[0] for s in sentence_tokenizer.span_tokenize(text)]\n",
    "    return [text[spans[i] : spans[i + 1]] for i in range(len(spans) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## embedding model via pgai and containerized vectordb\n",
    "\n",
    "\n",
    "# def create_vectorizer(embedding_model, embeddings_dimensions):\n",
    "#     embeddings_view_name = (\n",
    "#         f'{\"essays\"}{\"_\"}{embedding_model.replace(\"-\", \"_\")}{\"_\"}{\"embeddings\"}'\n",
    "#     )\n",
    "\n",
    "#     with connect_db() as conn:\n",
    "#         with conn.cursor() as cur:\n",
    "#             cur.execute(\n",
    "#                 f\"\"\"\n",
    "#                 SELECT ai.create_vectorizer(\n",
    "#                 'essays'::regclass,\n",
    "#                 destination => {embeddings_view_name},\n",
    "#                 embedding => ai.embedding_ollama({embedding_model}, {embeddings_dimensions}),\n",
    "#                 chunking => ai.chunking_recursive_character_text_splitter('text', {s.chunk_size}, {s.chunk_overlap}),\n",
    "#                 formatting => ai.formatting_python_template('title: $title $chunk')\n",
    "#                 );\"\"\"\n",
    "#             )\n",
    "\n",
    "\n",
    "# # with connect_db() as conn:\n",
    "# #    with conn.cursor() as cur:\n",
    "# #         cur.execute(\"\"\"\n",
    "# #             SELECT ai.load_dataset(\n",
    "# #                     'sgoel9/xxx_essays',\n",
    "# #                     table_name => 'essays',\n",
    "# #                     if_table_exists => 'append');\n",
    "# #         \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## preprocess documents (cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting text from pdfs using pdfminer\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "docs = []\n",
    "DOCS_DIR = \"../\" + s.settings.PATH_DATA + \"text_sources/\"\n",
    "\n",
    "for filename in os.listdir(DOCS_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(DOCS_DIR, filename)\n",
    "        text = extract_text(file_path)\n",
    "        print(text)\n",
    "        docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[2]  ## all docs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove reference section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean from headers+footers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_source_name = \"Koks et al - 2022 - Brief communication\"  # define which pdf should be read converted to txt\n",
    "# with open(\"../\" + s.PATH_DATA + f\"{text_source_name}.txt\", \"w+\") as f:\n",
    "#     json.dump(\n",
    "#         extract_text_from_pdf(\"../\" + s.PATH_DATA + f\"text_sources/{text_source_name}.pdf\"),\n",
    "#         f,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## fill vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "from uuid import UUID, uuid4\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class EntryTextSource():\n",
    "#     def __init__(self, title: str, source: str, content: str, authors: str = None, metadata: dict = None):\n",
    "#         self.title = title\n",
    "#         self.source = source\n",
    "#         self.content= content\n",
    "#         self.authors = authors\n",
    "#         self.metadata = metadata\n",
    "\n",
    "\n",
    "# ensure a fix structure for text source entries\n",
    "class TextSource(BaseModel):\n",
    "    id: UUID = Field(\n",
    "        default_factory=uuid4\n",
    "    )  # make unique entry id to prevent overwriting\n",
    "    title: str\n",
    "    source: str\n",
    "    content: str\n",
    "    authors: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    # make model immutable\n",
    "    model_config = ConfigDict(frozen=True)\n",
    "\n",
    "\n",
    "?TextSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_entry = {\n",
    "#     \"title\": \"test title\",\n",
    "#     \"authors\": None,\n",
    "#     \"source\": \"test source\",\n",
    "#     \"content\": \"test \",\n",
    "#     \"metadata\": {\"tags\": [\"ahr_valley\", \"scientific_publication\"], \"published_date\": \"2022-11-29\"}\n",
    "# }\n",
    "\n",
    "# TextSource(**test_entry).metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db():\n",
    "    conn = pg.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"postgres\",\n",
    "        dbname=\"postgres\",\n",
    "        port=\"5432\",\n",
    "        password=\"postgres\",\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "connect_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database and insert automatically all pdf files stored in data folder\n",
    "import json\n",
    "\n",
    "\n",
    "def fill_db(entry: TextSource):\n",
    "    curs.execute(\n",
    "        f\"\"\"\n",
    "        INSERT INTO text_source (title, authors, source, content, metadata) \n",
    "        VALUES\n",
    "        ('{entry.authors}',\n",
    "        '{entry.title}',\n",
    "        '{entry.source}',\n",
    "        '{entry.content}',\n",
    "        '{json.dumps(entry.metadata)}'\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill db automatically\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "conn = connect_db()\n",
    "curs = conn.cursor()\n",
    "\n",
    "DOCS_DIR = \"../\" + s.settings.PATH_DATA + \"text_sources/\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(DOCS_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(f\"fetching: {filename}\")\n",
    "\n",
    "        file_path = os.path.join(DOCS_DIR, filename)\n",
    "        text = extract_text(file_path)\n",
    "        filename = Path(filename).stem\n",
    "        authors, title = authors, title = (\n",
    "            re.compile(r\"(.+?)[0-9]{4}(.*)?\").search(filename).groups()\n",
    "        )\n",
    "\n",
    "        entry = {\n",
    "            \"authors\": authors.strip(),\n",
    "            \"title\": title.strip(),\n",
    "            \"source\": \"dummy source\",\n",
    "            \"content\": text,\n",
    "            \"metadata\": {\n",
    "                \"tags\": [\"ahr_valley\", \"dummy_publication_type\"],\n",
    "                \"published_date\": re.findall(r\"[0-9]{4}\", filename)[0],\n",
    "            },\n",
    "        }\n",
    "    fill_db(TextSource(**entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check entries\n",
    "conn = connect_db()\n",
    "curs = conn.cursor()\n",
    "\n",
    "curs.execute(\"SELECT * FROM nomic_embed_text_content_embeddings;\")\n",
    "rows = curs.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Clean up\n",
    "curs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Mohr 2022 A multi-disciplinary analysis of the exceptional flood event of July 2021 in central Europe - Part 1 Event desciption and analysis\"\n",
    "\n",
    "authors, title = re.compile(r\"(.+?)[0-9]{4}(.*)?\").search(filename).groups()\n",
    "# authors, title = re.compile(r\"(.*)[0-9]{4}(.*)?\").search(filename).groups()\n",
    "authors, title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Load content from vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the context text from the response\n",
    "context = \"\".join(context_response[\"context\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## connect to postgres DB to receive context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_db(query):\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    records = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_db(\"SELECT chunk FROM text_source_content_embeddings;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## load decoder model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0  # nvidia gpu\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "# %env TORCH_CUDA_ARCH_LIST=8.6\n",
    "\n",
    "# settings for distributed computing\n",
    "%env WORLD_SIZE=1\n",
    "%env RANK=0\n",
    "%env LOCAL_RANK=0\n",
    "\n",
    "# NOTE: # WORLD_SIZE: each GPU corresponds to one process (world = no. of processes within a group), processes communicate with each other enabling eg., distributed training\n",
    "# NOTE: # RANK: IDs of the processes, ranging from 0 up to WORLD_SIZE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check env-vars\n",
    "# %env PYTORCH_CUDA_ALLOC_CONF\n",
    "# os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# set default location to store models\n",
    "os.environ[\"HF_HOME\"] = (\n",
    "    \"/home/a-buch/Documents/_PROJECTS/CI-impacts-information-retrieval/notebooks/huggingface_mirror/\"\n",
    ")\n",
    "\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GPTJForQuestionAnswering,\n",
    ")\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block needs only need to be adapted for cluster\n",
    "# # for own laptop use random port number and localhost (127.0.0.1) as placeholder\n",
    "\n",
    "%env MASTER_ADDR=127.0.0.1\n",
    "%env MASTER_PORT=6006\n",
    "\n",
    "# # Initialize distributed computing\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "device = torch.device(f\"cuda:{rank}\")\n",
    "torch.cuda.set_device(device)\n",
    "torch.distributed.init_process_group(backend=\"nccl\")\n",
    "# # torch.distributed.init_process_group(backend='nccl', init_method='env://', rank = torch.cuda.device_count(), world_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check cuda device number and ids\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(\"GPU: \", i, torch.cuda.get_device_name(i))  # get current device name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Abstract. Germany, Belgium and the Netherlands were hit\n",
    "by extreme precipitation and flooding in July 2021. This\n",
    "brief communication provides an overview of the impacts to\n",
    "large-scale critical infrastructure systems and how recovery\n",
    "has progressed. The results show that Germany and Belgium\n",
    "were particularly affected, with many infrastructure assets\n",
    "severely damaged or completely destroyed. Impacts range\n",
    "from completely destroyed bridges and sewage systems, to\n",
    "severely damaged schools and hospitals. We find that (largescale)\n",
    "risk assessments, often focused on larger (river) flood\n",
    "events, do not find these local, but severe, impacts due to critical\n",
    "infrastructure failures. This may be the result of limited\n",
    "availability of validation material. As such, this brief communication\n",
    "not only will help to better understand how critical\n",
    "infrastructure can be affected by flooding, but also can be\n",
    "used as validation material for future flood risk assessments.\\n\\n\n",
    "1 Introduction\n",
    "In mid-July 2021, a persistent low-pressure system caused\n",
    "extreme precipitation in parts of the Belgian, German and\n",
    "Dutch catchments of the Meuse and Rhine rivers. This led\n",
    "to record-breaking water levels and severe flooding (Mohr\n",
    "et al., 2022). Comparable heavy precipitation events in this\n",
    "area have never been registered in most of the affected areas\n",
    "before (Kreienkamp et al., 2021). The German states most affected\n",
    "include Rhineland-Palatinate (Rheinland-Pfalz), with\n",
    "damage to the Ahr River valley (Ahrtal), several regions in\n",
    "the Eiffel National Park, to the city of Trier. Flooding in\n",
    "Belgium was concentrated in the Vesdre River valley (districts\n",
    "of Pepinster, Ensival and Verviers), the Meuse River\n",
    "valley (Maaseik, Liége), the Gete River valley (Herk-de-Stad\n",
    "and Halen) and southeast Brussels (Wavre). The Netherlands\n",
    "experienced flooding, mostly concentrated in the southern\n",
    "district of Limburg. In total, at least 220 casualties have\n",
    "been reported, with insured loss estimates of approximately\n",
    "EUR 150 million–EUR 250 million in the Netherlands (Verbond\n",
    "voor Verzekeraars, 2022), EUR 2.2 billion in Belgium\n",
    "(Assuralia, 2022) and EUR 8.2 billion (GDV, 2022)\n",
    "in Germany. The event caused major damages to residential\n",
    "and commercial structures and to many critical infrastructure\n",
    "(CI) assets. Not only vital functions for first responders\n",
    "were affected (e.g. hospitals, fire departments), but also railways,\n",
    "bridges and utility networks (e.g. water and electricity\n",
    "supply) were severely damaged, expecting to take months to\n",
    "years to fully rebuild. \\n\\n\n",
    "CI is often considered to be the backbone of a wellfunctioning\n",
    "society (Hall et al., 2016), which is particularly\n",
    "eminent during natural hazards and disasters. For instance,\n",
    "failure of electricity or telecommunication services immediately\n",
    "causes disruptions in the day-to-day functioning of people\n",
    "and businesses, including those outside the directly affected\n",
    "area. Despite the (academic) agreement that failure of\n",
    "infrastructure systems may cause (large-scale) societal disruptions\n",
    "(Garschagen and Sandholz, 2018; Hallegatte et al.,\n",
    "2019; Fekete and Sandholz, 2021), empirical evidence on the\n",
    "impacts of extreme weather events on these systems is still\n",
    "Published by Copernicus Publications on behalf of the European Geosciences Union.\n",
    "3832 E. E. Koks et al.: Flood impacts to infrastructure\n",
    "limited. \\n\\n This brief communication provides an overview of\n",
    "the observed flood impacts to large-scale infrastructure systems\n",
    "during the 2021 mid-July western European flood event\n",
    "and how reconstruction of these large-scale systems has progressed.\n",
    "Next, we highlight how some of these observations\n",
    "compare to academic modelling approaches. We conclude\n",
    "with suggestions on moving forward in CI risk modelling,\n",
    "based on the lessons learned from this extreme event. \\n\\n\n",
    "2 Critical infrastructure impacts\n",
    "2.1 Transport infrastructure\n",
    "In Germany, road and railway infrastructure was severely\n",
    "damaged as documented exemplarily in Fig. 1. Cost estimates\n",
    "reach up to EURO2 billion Euro (MDR, 2021). More\n",
    "than 130 km of motorways were closed directly after the\n",
    "event, of which 50 km were still closed two months later,\n",
    "with an estimated repair cost of EUR100 million (Hauser,\n",
    "2021). Of the 112 bridges in the flooded 40 km of the Ahr\n",
    "valley (Rhineland-Palatinate), 62 bridges were destroyed,\n",
    "13 were severely damaged and only 35 were in operation\n",
    "a month after the flood event (MDR, 2021). Over 74 km\n",
    "of roads, paths and bridges in the Ahr valley have been\n",
    "(critically) damaged. In some cases, repairs are expected to\n",
    "take months to years (Zeit Online, 2021). For example, major\n",
    "freeway sections, including parts of the A1 motorway,\n",
    "were closed until early 2022 (24Rhein, 2022). In addition,\n",
    "about 50 000 cars were damaged, causing insurance claims of\n",
    "some EUR 450 million (ADAC, 2021). The German railway\n",
    "provider Deutsche Bahn expects asset damages of around\n",
    "EUR 1.3 billion. Among other things, 180 level crossings,\n",
    "almost 40 signal boxes, over 1000 catenary and signal masts,\n",
    "and 600 km of tracks were destroyed, as well as energy supply\n",
    "systems, elevators and lighting systems (MDR, 2021).\n",
    "As of 11 April 2022, 14 of the affected rail stretches are\n",
    "fully functional again. The less damaged stretches were functional\n",
    "again within 3 months, while some of the most damaged\n",
    "sections in the Ahr valley are expected to be finished\n",
    "by the end of 2025 (DB, 2022). In Belgium, approximately\n",
    "10 km of railway tracks and 3000 sleeper tracks have to be replaced;\n",
    "50 km of catenary needs to be repaired; and 70 000 t\n",
    "of railway track bed needs to be placed, with estimated\n",
    "costs between EUR 30 million–EUR 50 million (Rozendaal,\n",
    "2021a). Most damages have been repaired within 2 weeks.\n",
    "The most severely damaged railway line (between the villages\n",
    "of Spa and Pepinster) was reopened again on 3 October\n",
    "2021 (Rozendaal, 2021b). In the Netherlands, no largescale\n",
    "damage has been reported to transport infrastructure. A\n",
    "few national highways were partly flooded (e.g. the A76 in\n",
    "both directions) or briefly closed (<3 d) because of the potential\n",
    "of flooding. \\n\n",
    "Most likely due to relative low-flow velocities,\n",
    "damage to Dutch national road infrastructure was\n",
    "limited. Several railway sections were closed (e.g. the railway\n",
    "section between Maastricht and Liége) and some damage\n",
    "occurred to the railway infrastructure, in particular to the\n",
    "electronic “track circuit” devices and saturated railway embankments\n",
    "(Prorail, 2021).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Which societal or economic impacts of infrastructure failures are mentioned in the text?\"\n",
    "\n",
    "question = \"Which impacts of infrastructure failures are mentioned in the text? Categorize the output by the type of infrastructure, societal or economic impacts, the location and possibly the time of the infrastructure failure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # https://github.com/huggingface/transformers/issues/12448\n",
    "\n",
    "# # Download model and tokenizer\n",
    "# model_name = \"EleutherAI/gpt-j-6B\" # \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# base_dir = \"./huggingface_mirror\"\n",
    "# model_dir = base_dir + \"/hub/\"\n",
    "\n",
    "# # Run once to download the model and cache it locally\n",
    "# snapshot_download(\n",
    "#     repo_id=\"EleutherAI/gpt-j-6B\", # \"meta-llama/Llama-2-7b-chat-hf\",  # \"google/gemma-3-4b-it\",\n",
    "#     cache_dir=model_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### Test GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "# # empyty CUDA cache\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init class for decoder and tokenizer\n",
    "\n",
    "\n",
    "# class DecoderModel:\n",
    "#     def __init__(self):\n",
    "#         login(\n",
    "#             token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "#         )  # TODO replace by using pydantic settings\n",
    "\n",
    "#         # model_name = \"google/gemma-3-4b-it\" # \"kallidavidson/TinyBERT_General_4L_312D\"  # \"huawei-noah/TinyBERT_General_4L_312D\" # - for QA - less DWL\n",
    "#         # model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#         model_name = \"EleutherAI/gpt-j-6B\" #\"distilbert-base-multilingual-cased\"\n",
    "#         base_dir = \"./huggingface_mirror\"  # use default dir in .cache/\n",
    "#         model_dir = base_dir + \"/hub/\"  # + \"models--\" + model_name.replace(\"/\", \"--\")\n",
    "#         print(model_dir)\n",
    "\n",
    "#         # quantization config\n",
    "#         # Load model with 4-bit quantization if applicable (use 4-bit integer instead of 32b floats) --> reduce the required VRAM for model application\n",
    "#         # see, https://huggingface.co/docs/transformers/quantization\n",
    "#         bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.float16,\n",
    "#         )\n",
    "\n",
    "#         self.pipeline, self.tokenizer = self.initialize_model(\n",
    "#             model_name, model_dir, bnb_config\n",
    "#         )\n",
    "\n",
    "#     def initialize_model(self, model_name: str, model_dir: str = None, bnb_config=None):\n",
    "\n",
    "#         # Model and Tokenizer initialization\n",
    "#         if not os.path.exists(model_dir):\n",
    "#             print(\"Model directory not found. Downloading model...\")\n",
    "#             os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#             device = transformers.infer_device()\n",
    "#             print(f\"Using device: {device}\")\n",
    "#             model = GPTJForQuestionAnswering.from_pretrained(\n",
    "#                 model_name,\n",
    "#                 dtype=\"auto\",\n",
    "#                 attn_implementation=\"flash_attention_2\",  # use with 4-bit quantization,\n",
    "#                 # --> flash attention enables to use much larger sequence lengths without running into OOM issues\n",
    "#                 quantization_config=bnb_config,\n",
    "#                 # max_memory={0: \"2GB\", 1: \"10GB\"},  # distribute memory across GPUs\n",
    "#                 tp_plan=\"auto\",\n",
    "#             )\n",
    "#             model.save_pretrained(model_dir)\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "#             tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "#             print(\"Downloaded model and tokenizer\")\n",
    "\n",
    "#         else:\n",
    "#             print(f\"Using locally saved model from {model_dir}\")\n",
    "\n",
    "#             model = GPTJForQuestionAnswering.from_pretrained(\n",
    "#                 model_name,\n",
    "#                 cache_dir=model_dir,\n",
    "#                 local_files_only=True,  # tp_plan=\"auto\" # set tensor parallel model (ie. splits model on multiple GPU)\n",
    "#                 # dtype=\"auto\",\n",
    "#                 dtype=torch.float16,\n",
    "#                 attn_implementation=\"flash_attention_2\",  # use with 4-bit quantization,\n",
    "#                 # --> flash attention enables to use much larger sequence lengths without running into OOM issues\n",
    "#                 quantization_config=bnb_config,\n",
    "#                 # max_memory={0: \"2GB\", 1: \"10GB\"},  # distribute memory across GPUs\n",
    "#                 tp_plan=\"auto\",  # automatically use a tensor parallelism plan based on predefined configuration of the model (i.e. partition model on both GPUs)\n",
    "#             )\n",
    "#             print(\"Tensor parallel plan:\", model._tp_plan)\n",
    "\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(\n",
    "#                 model_name, use_fast=True, cache_dir=model_dir, # use fast Rust-based tokenizer, when possible\n",
    "#             )\n",
    "\n",
    "#         # reduce further memory usage\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         model = model.to(device)\n",
    "#         model.use_checkpointing = True\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pipeline setup for question answering\n",
    "#         pipeline = transformers.pipeline(  # load model locally from wsl .cache\\\n",
    "#             \"question-answering\",  # task defining which pipeline is returned\n",
    "#             #\"text-generation\",\n",
    "#             model=model,\n",
    "#             tokenizer=tokenizer, #(return_tensors=\"pt\"),  # load specific tokenizer based on model-name (via AutoTokenizer) ensuring text is tokenized in accordance to the way the model was trained\n",
    "#             max_new_tokens=256,\n",
    "#             dtype=torch.float16,\n",
    "#             # low_cpu_mem_usage=True,\n",
    "#             device_map=\"auto\",\n",
    "#         )\n",
    "#         return pipeline, tokenizer\n",
    "\n",
    "#     def generate_response(self, question: str, context: str):\n",
    "#         # Preparing the input prompts\n",
    "#         prompt = {\"question\": question, \"context\": context}\n",
    "#         # messages = [\n",
    "#         #     {\"role\": \"system\", \"content\": context},\n",
    "#         #     {\"role\": \"user\", \"content\": question},\n",
    "#         # ]\n",
    "#         # # Combine messages into a single string prompt\n",
    "#         # prompt = \"\\n\".join([f'{msg[\"role\"]}: {msg[\"content\"]}' for msg in messages])\n",
    "#         # print(\"prompt:\", messages[1][\"content\"])\n",
    "\n",
    "#         # Generating responses\n",
    "#         sequences = self.pipeline(\n",
    "#             prompt,  # for text generation\n",
    "#             # question=question, context=context,  # for eQA\n",
    "#             max_new_tokens=256,\n",
    "#             do_sample=True,\n",
    "#             eos_token_id=self.tokenizer.eos_token_id,\n",
    "#         )\n",
    "#         # Extracting and returning the generated text\n",
    "#         return sequences\n",
    "\n",
    "\n",
    "# decoder_model = DecoderModel()\n",
    "# response = decoder_model.generate_response(question=question, context=context)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "\n",
    "Abstract. Germany, Belgium and the Netherlands were hit\n",
    "by extreme precipitation and flooding in July 2021. This\n",
    "brief communication provides an overview of the impacts to\n",
    "large-scale critical infrastructure systems and how recovery\n",
    "has progressed. The results show that Germany and Belgium\n",
    "were particularly affected, with many infrastructure assets\n",
    "severely damaged or completely destroyed. Impacts range\n",
    "from completely destroyed bridges and sewage systems, to\n",
    "severely damaged schools and hospitals. We find that (largescale)\n",
    "risk assessments, often focused on larger (river) flood\n",
    "events, do not find these local, but severe, impacts due to critical\n",
    "infrastructure failures. This may be the result of limited\n",
    "availability of validation material. As such, this brief communication\n",
    "not only will help to better understand how critical\n",
    "infrastructure can be affected by flooding, but also can be\n",
    "used as validation material for future flood risk assessments.\n",
    "\n",
    "\n",
    "1 Introduction\n",
    "In mid-July 2021, a persistent low-pressure system caused\n",
    "extreme precipitation in parts of the Belgian, German and\n",
    "Dutch catchments of the Meuse and Rhine rivers. This led\n",
    "to record-breaking water levels and severe flooding (Mohr\n",
    "et al., 2022). Comparable heavy precipitation events in this\n",
    "area have never been registered in most of the affected areas\n",
    "before (Kreienkamp et al., 2021). The German states most affected\n",
    "include Rhineland-Palatinate (Rheinland-Pfalz), with\n",
    "damage to the Ahr River valley (Ahrtal), several regions in\n",
    "the Eiffel National Park, to the city of Trier. Flooding in\n",
    "Belgium was concentrated in the Vesdre River valley (districts\n",
    "of Pepinster, Ensival and Verviers), the Meuse River\n",
    "valley (Maaseik, Liége), the Gete River valley (Herk-de-Stad\n",
    "and Halen) and southeast Brussels (Wavre). The Netherlands\n",
    "experienced flooding, mostly concentrated in the southern\n",
    "district of Limburg. In total, at least 220 casualties have\n",
    "been reported, with insured loss estimates of approximately\n",
    "EUR 150 million–EUR 250 million in the Netherlands (Verbond\n",
    "voor Verzekeraars, 2022), EUR 2.2 billion in Belgium\n",
    "(Assuralia, 2022) and EUR 8.2 billion (GDV, 2022)\n",
    "in Germany. The event caused major damages to residential\n",
    "and commercial structures and to many critical infrastructure\n",
    "(CI) assets. Not only vital functions for first responders\n",
    "were affected (e.g. hospitals, fire departments), but also railways,\n",
    "bridges and utility networks (e.g. water and electricity\n",
    "supply) were severely damaged, expecting to take months to\n",
    "years to fully rebuild. \n",
    "\n",
    "\n",
    "CI is often considered to be the backbone of a wellfunctioning\n",
    "society (Hall et al., 2016), which is particularly\n",
    "eminent during natural hazards and disasters. For instance,\n",
    "failure of electricity or telecommunication services immediately\n",
    "causes disruptions in the day-to-day functioning of people\n",
    "and businesses, including those outside the directly affected\n",
    "area. Despite the (academic) agreement that failure of\n",
    "infrastructure systems may cause (large-scale) societal disruptions\n",
    "(Garschagen and Sandholz, 2018; Hallegatte et al.,\n",
    "2019; Fekete and Sandholz, 2021), empirical evidence on the\n",
    "impacts of extreme weather events on these systems is still\n",
    "Published by Copernicus Publications on behalf of the European Geosciences Union.\n",
    "3832 E. E. Koks et al.: Flood impacts to infrastructure\n",
    "limited. \n",
    "\n",
    " This brief communication provides an overview of\n",
    "the observed flood impacts to large-scale infrastructure systems\n",
    "during the 2021 mid-July western European flood event\n",
    "and how reconstruction of these large-scale systems has progressed.\n",
    "Next, we highlight how some of these observations\n",
    "compare to academic modelling approaches. We conclude\n",
    "with suggestions on moving forward in CI risk modelling,\n",
    "based on the lessons learned from this extreme event. \n",
    "\n",
    "\n",
    "2 Critical infrastructure impacts\n",
    "2.1 Transport infrastructure\n",
    "In Germany, road and railway infrastructure was severely\n",
    "damaged as documented exemplarily in Fig. 1. Cost estimates\n",
    "reach up to EURO2 billion Euro (MDR, 2021). More\n",
    "than 130 km of motorways were closed directly after the\n",
    "event, of which 50 km were still closed two months later,\n",
    "with an estimated repair cost of EUR100 million (Hauser,\n",
    "2021). Of the 112 bridges in the flooded 40 km of the Ahr\n",
    "valley (Rhineland-Palatinate), 62 bridges were destroyed,\n",
    "13 were severely damaged and only 35 were in operation\n",
    "a month after the flood event (MDR, 2021). Over 74 km\n",
    "of roads, paths and bridges in the Ahr valley have been\n",
    "(critically) damaged. In some cases, repairs are expected to\n",
    "take months to years (Zeit Online, 2021). For example, major\n",
    "freeway sections, including parts of the A1 motorway,\n",
    "were closed until early 2022 (24Rhein, 2022). In addition,\n",
    "about 50 000 cars were damaged, causing insurance claims of\n",
    "some EUR 450 million (ADAC, 2021). The German railway\n",
    "provider Deutsche Bahn expects asset damages of around\n",
    "EUR 1.3 billion. Among other things, 180 level crossings,\n",
    "almost 40 signal boxes, over 1000 catenary and signal masts,\n",
    "and 600 km of tracks were destroyed, as well as energy supply\n",
    "systems, elevators and lighting systems (MDR, 2021).\n",
    "As of 11 April 2022, 14 of the affected rail stretches are\n",
    "fully functional again. The less damaged stretches were functional\n",
    "again within 3 months, while some of the most damaged\n",
    "sections in the Ahr valley are expected to be finished\n",
    "by the end of 2025 (DB, 2022). In Belgium, approximately\n",
    "10 km of railway tracks and 3000 sleeper tracks have to be replaced;\n",
    "50 km of catenary needs to be repaired; and 70 000 t\n",
    "of railway track bed needs to be placed, with estimated\n",
    "costs between EUR 30 million–EUR 50 million (Rozendaal,\n",
    "2021a). Most damages have been repaired within 2 weeks.\n",
    "The most severely damaged railway line (between the villages\n",
    "of Spa and Pepinster) was reopened again on 3 October\n",
    "2021 (Rozendaal, 2021b). In the Netherlands, no largescale\n",
    "damage has been reported to transport infrastructure. A\n",
    "few national highways were partly flooded (e.g. the A76 in\n",
    "both directions) or briefly closed (<3 d) because of the potential\n",
    "of flooding. \n",
    "\n",
    "Most likely due to relative low-flow velocities,\n",
    "damage to Dutch national road infrastructure was\n",
    "limited. Several railway sections were closed (e.g. the railway\n",
    "section between Maastricht and Liége) and some damage\n",
    "occurred to the railway infrastructure, in particular to the\n",
    "electronic “track circuit” devices and saturated railway embankments\n",
    "(Prorail, 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpcloud\n",
    "\n",
    "client = nlpcloud.Client(\"gpt-j\", \"your_token\", gpu=True)\n",
    "\n",
    "generation = client.generation(\n",
    "    f\"\"\"\n",
    "    Context: More than 130 km of motorways were closed directly after the event, of which 50 km were still closed two months later, with an estimated repair cost of EUR100 million (Hauser, 2021). Of the 112 bridges in the flooded 40 km of the Ahr valley (Rhineland-Palatinate), 62 bridges were destroyed, 13 were severely damaged and only 35 were in operation a month after the flood event (MDR, 2021).\n",
    "    Question: How many bridges were destroyed in the Ahr valley during the 2021 flood event?\n",
    "    Answer: 62\n",
    "    ###\n",
    "    Context: More than 130 km of motorways were closed directly after the event, of which 50 km were still closed two months later, with an estimated repair cost of EUR100 million (Hauser, 2021). Of the 112 bridges in the flooded 40 km of the Ahr valley (Rhineland-Palatinate), 62 bridges were destroyed, 13 were severely damaged and only 35 were in operation a month after the flood event (MDR, 2021).\n",
    "    Question: How many bridges were in operation a month after the flood event in the Ahr valley?\n",
    "    Answer: 35\n",
    "    ###\n",
    "    Context: More than 130 km of motorways were closed directly after the event, of which 50 km were still closed two months later, with an estimated repair cost of EUR100 million (Hauser, 2021). Of the 112 bridges in the flooded 40 km of the Ahr valley (Rhineland-Palatinate), 62 bridges were destroyed, 13 were severely damaged and only 35 were in operation a month after the flood event (MDR, 2021).\n",
    "    Question: How many bridges were at least affected by the flood event in the Ahr valley?\n",
    "    Answer: 77\n",
    "    ###\n",
    "    Context:  In total, at least 220 casualties have been reported, with insured loss estimates of approximately EUR 150 million–EUR 250 million in the Netherlands (Verbond voor Verzekeraars, 2022), EUR 2.2 billion in Belgium (Assuralia, 2022) and EUR 8.2 billion (GDV, 2022) in Germany. The event caused major damages to residential and commercial structures and to many critical infrastructure (CI) assets. \n",
    "    Question: How high are the estimated insured losses in Germany?\n",
    "    Answer: EUR 8.2 billion\n",
    "    ###\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    # min_length=1,\n",
    "    max_length=20,\n",
    "    length_no_input=True,\n",
    "    end_sequence=\"###\",\n",
    "    remove_end_sequence=True,\n",
    "    remove_input=True,\n",
    ")\n",
    "print(generation[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import traceback\n",
    "\n",
    "\n",
    "# traceback.clear_frames(sys.last_traceback)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# try:\n",
    "#     a = 1/0\n",
    "# except Exception as e:\n",
    "#     exc_tuple = sys.exc_info()\n",
    "#     print(e, exc_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import traceback\n",
    "\n",
    "# traceback.clear_frames(sys.last_traceback)\n",
    "\n",
    "## empty CUDA cache\n",
    "\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### Test llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init class for decoder and tokenizer\n",
    "\n",
    "\n",
    "class DecoderModel:\n",
    "    def __init__(self):\n",
    "        login(\n",
    "            token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "        )  # TODO replace by using pydantic settings\n",
    "\n",
    "        # model_name = \"google/gemma-3-4b-it\" # \"kallidavidson/TinyBERT_General_4L_312D\"  # \"huawei-noah/TinyBERT_General_4L_312D\" # - for QA - less DWL\n",
    "        model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        # \"distilbert-base-multilingual-cased\"\n",
    "        base_dir = \"./huggingface_mirror\"  # use default dir in .cache/\n",
    "        model_dir = base_dir + \"/hub/\"  # + \"models--\" + model_name.replace(\"/\", \"--\")\n",
    "        print(model_dir)\n",
    "\n",
    "        # quantization config\n",
    "        # Load model with 4-bit quantization if applicable (use 4-bit integer instead of 32b floats) --> reduce the required VRAM for model application\n",
    "        # see, https://huggingface.co/docs/transformers/quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "        self.pipeline, self.tokenizer = self.initialize_model(\n",
    "            model_name, model_dir, bnb_config\n",
    "        )\n",
    "\n",
    "    def initialize_model(self, model_name: str, model_dir: str = None, bnb_config=None):\n",
    "\n",
    "        # Model and Tokenizer initialization\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(\"Model directory not found. Downloading model...\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            device = transformers.infer_device()\n",
    "            print(f\"Using device: {device}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                local_files_only=True,  # tp_plan=\"auto\" # set tensor parallel model (ie. splits model on multiple GPU)\n",
    "                dtype=\"auto\",\n",
    "                attn_implementation=\"flash_attention_2\",  # use with 4-bit quantization,\n",
    "                # --> flash attention enables to use much larger sequence lengths without running into OOM issues\n",
    "                quantization_config=bnb_config,\n",
    "                # max_memory={0: \"2GB\", 1: \"10GB\"},  # distribute memory across GPUs\n",
    "                tp_plan=\"auto\",\n",
    "            )\n",
    "            model.save_pretrained(model_dir)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "            tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "            print(\"Downloaded model and tokenizer\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Using locally saved model from {model_dir}\")\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=model_dir,\n",
    "                local_files_only=True,  # tp_plan=\"auto\" # set tensor parallel model (ie. splits model on multiple GPU)\n",
    "                dtype=\"auto\",\n",
    "                attn_implementation=\"flash_attention_2\",  # use with 4-bit quantization,\n",
    "                # --> flash attention enables to use much larger sequence lengths without running into OOM issues\n",
    "                quantization_config=bnb_config,\n",
    "                tp_plan=\"auto\",  # automatically use a tensor parallelism plan based on predefined configuration of the model (i.e. partition model on both GPUs)\n",
    "            )\n",
    "            print(\"Tensor parallel plan:\", model._tp_plan)\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                use_fast=True,\n",
    "                cache_dir=model_dir,  # use fast Rust-based tokenizer, when possible\n",
    "            )\n",
    "\n",
    "        # reduce further memory usage\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        model.use_checkpointing = True\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Pipeline setup for question answering\n",
    "        pipeline = transformers.pipeline(  # load model locally from wsl .cache\\\n",
    "            \"text-generation\",\n",
    "            # \"question-answering\",  # task defining which pipeline is returned\n",
    "            model=model,\n",
    "            tokenizer=tokenizer(\n",
    "                return_tensors=\"pt\"\n",
    "            ),  # load specific tokenizer based on model-name (via AutoTokenizer) ensuring text is tokenized in accordance to the way the model was trained\n",
    "            max_new_tokens=256,\n",
    "            # low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        return pipeline, tokenizer\n",
    "\n",
    "    def generate_response(self, question: str, context: str):\n",
    "        # Preparing the input prompts\n",
    "        # prompt = {\"question\": question, \"context\": context}\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        # Combine messages into a single string prompt\n",
    "        prompt = \"\\n\".join([f'{msg[\"role\"]}: {msg[\"content\"]}' for msg in messages])\n",
    "        print(\"prompt:\", messages[1][\"content\"])\n",
    "\n",
    "        # Generating responses\n",
    "        sequences = self.pipeline(\n",
    "            prompt,  # for text generation\n",
    "            # question=question, context=context,  # for eQA\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            # top_k=10,\n",
    "            # top_p=0.5,\n",
    "            # num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        # Extracting and returning the generated text\n",
    "        return sequences\n",
    "\n",
    "\n",
    "decoder_model = DecoderModel()\n",
    "response = decoder_model.generate_response(question=question, context=context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "# # empyty CUDA cache\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0].keys())\n",
    "print(response[0][\"generated_text\"].split(\"user: \")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### check response versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "[{'generated_text': 'system: \\nAbstract. Germany, Belgium and the Netherlands were hit\\nby extreme precipitation and flooding in July 2021. This\\nbrief communication provides an overview of the impacts to\\nlarge-scale critical infrastructure systems and how recovery\\nhas progressed. The results show that Germany and Belgium\\nwere particularly affected, with many infrastructure assets\\nseverely damaged or completely destroyed. Impacts range\\nfrom completely destroyed bridges and sewage systems, to\\nseverely damaged schools and hospitals. We find that (largescale)\\nrisk assessments, often focused on larger (river) flood\\nevents, do not find these local, but severe, impacts due to critical\\ninfrastructure failures. This may be the result of limited\\navailability of validation material. As such, this brief communication\\nnot only will help to better understand how critical\\ninfrastructure can be affected by flooding, but also can be\\nused as validation material for future flood risk assessments.\\n\\n\\n1 Introduction\\nIn mid-July 2021, a persistent low-pressure system caused\\nextreme precipitation in parts of the Belgian, German and\\nDutch catchments of the Meuse and Rhine rivers. This led\\nto record-breaking water levels and severe flooding (Mohr\\net al., 2022). Comparable heavy precipitation events in this\\narea have never been registered in most of the affected areas\\nbefore (Kreienkamp et al., 2021). The German states most affected\\ninclude Rhineland-Palatinate (Rheinland-Pfalz), with\\ndamage to the Ahr River valley (Ahrtal), several regions in\\nthe Eiffel National Park, to the city of Trier. Flooding in\\nBelgium was concentrated in the Vesdre River valley (districts\\nof Pepinster, Ensival and Verviers), the Meuse River\\nvalley (Maaseik, Liége), the Gete River valley (Herk-de-Stad\\nand Halen) and southeast Brussels (Wavre). The Netherlands\\nexperienced flooding, mostly concentrated in the southern\\ndistrict of Limburg. In total, at least 220 casualties have\\nbeen reported, with insured loss estimates of approximately\\nEUR 150 million–EUR 250 million in the Netherlands (Verbond\\nvoor Verzekeraars, 2022), EUR 2.2 billion in Belgium\\n(Assuralia, 2022) and EUR 8.2 billion (GDV, 2022)\\nin Germany. The event caused major damages to residential\\nand commercial structures and to many critical infrastructure\\n(CI) assets. Not only vital functions for first responders\\nwere affected (e.g. hospitals, fire departments), but also railways,\\nbridges and utility networks (e.g. water and electricity\\nsupply) were severely damaged, expecting to take months to\\nyears to fully rebuild. \\n\\n\\nCI is often considered to be the backbone of a wellfunctioning\\nsociety (Hall et al., 2016), which is particularly\\neminent during natural hazards and disasters. For instance,\\nfailure of electricity or telecommunication services immediately\\ncauses disruptions in the day-to-day functioning of people\\nand businesses, including those outside the directly affected\\narea. Despite the (academic) agreement that failure of\\ninfrastructure systems may cause (large-scale) societal disruptions\\n(Garschagen and Sandholz, 2018; Hallegatte et al.,\\n2019; Fekete and Sandholz, 2021), empirical evidence on the\\nimpacts of extreme weather events on these systems is still\\nPublished by Copernicus Publications on behalf of the European Geosciences Union.\\n3832 E. E. Koks et al.: Flood impacts to infrastructure\\nlimited. \\n\\n This brief communication provides an overview of\\nthe observed flood impacts to large-scale infrastructure systems\\nduring the 2021 mid-July western European flood event\\nand how reconstruction of these large-scale systems has progressed.\\nNext, we highlight how some of these observations\\ncompare to academic modelling approaches. We conclude\\nwith suggestions on moving forward in CI risk modelling,\\nbased on the lessons learned from this extreme event. \\n\\n\\n2 Critical infrastructure impacts\\n2.1 Transport infrastructure\\nIn Germany, road and railway infrastructure was severely\\ndamaged as documented exemplarily in Fig. 1. Cost estimates\\nreach up to EURO2 billion Euro (MDR, 2021). More\\nthan 130 km of motorways were closed directly after the\\nevent, of which 50 km were still closed two months later,\\nwith an estimated repair cost of EUR100 million (Hauser,\\n2021). Of the 112 bridges in the flooded 40 km of the Ahr\\nvalley (Rhineland-Palatinate), 62 bridges were destroyed,\\n13 were severely damaged and only 35 were in operation\\na month after the flood event (MDR, 2021). Over 74 km\\nof roads, paths and bridges in the Ahr valley have been\\n(critically) damaged. In some cases, repairs are expected to\\ntake months to years (Zeit Online, 2021). For example, major\\nfreeway sections, including parts of the A1 motorway,\\nwere closed until early 2022 (24Rhein, 2022). In addition,\\nabout 50 000 cars were damaged, causing insurance claims of\\nsome EUR 450 million (ADAC, 2021). The German railway\\nprovider Deutsche Bahn expects asset damages of around\\nEUR 1.3 billion. Among other things, 180 level crossings,\\nalmost 40 signal boxes, over 1000 catenary and signal masts,\\nand 600 km of tracks were destroyed, as well as energy supply\\nsystems, elevators and lighting systems (MDR, 2021).\\nAs of 11 April 2022, 14 of the affected rail stretches are\\nfully functional again. The less damaged stretches were functional\\nagain within 3 months, while some of the most damaged\\nsections in the Ahr valley are expected to be finished\\nby the end of 2025 (DB, 2022). In Belgium, approximately\\n10 km of railway tracks and 3000 sleeper tracks have to be replaced;\\n50 km of catenary needs to be repaired; and 70 000 t\\nof railway track bed needs to be placed, with estimated\\ncosts between EUR 30 million–EUR 50 million (Rozendaal,\\n2021a). Most damages have been repaired within 2 weeks.\\nThe most severely damaged railway line (between the villages\\nof Spa and Pepinster) was reopened again on 3 October\\n2021 (Rozendaal, 2021b). In the Netherlands, no largescale\\ndamage has been reported to transport infrastructure. A\\nfew national highways were partly flooded (e.g. the A76 in\\nboth directions) or briefly closed (<3 d) because of the potential\\nof flooding. \\n\\nMost likely due to relative low-flow velocities,\\ndamage to Dutch national road infrastructure was\\nlimited. Several railway sections were closed (e.g. the railway\\nsection between Maastricht and Liége) and some damage\\noccurred to the railway infrastructure, in particular to the\\nelectronic “track circuit” devices and saturated railway embankments\\n(Prorail, 2021).\\nuser: Which impacts of infrastructure failures are mentioned in the text? Categorize the output by the type of infrastructure, societal or economic impacts, the location and possibly the time of the infrastructure failure.'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(response)):\n",
    "    # print(f\"{response[i]['generated_text']}\" ) # \\nscore: {response[i]['score']}\")\n",
    "    print(response[i][\"generated_text\"].split(\"assistant:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user: Which societal or economic impacts of infrastructure failures are mentioned in the text?\n",
    "\n",
    "# assistant: In Germany, the most severe impacts of the floods on critical infrastructure were reported in the Ahr valley and the Rhine river valley. These impacts included the destruction of infrastructure assets (e.g. Bridges, railway infrastructure) and severe damages to residential and commercial structures. CI infrastructure such as water and electricity supply and telecommunication networks were severely damaged, with estimated costs of EUR 150 million–EUR 250 million in the Netherlands (Verbond voor Verzekeraars, 2022). The floods also impacted the availability of water, sewage and wastewater services, and resulted in significant power outages for a short period. In Belgium, significant damage was reported to railway infrastructure, including the destruction of track bed and sleepers, while the most severe impacts were reported in the Ahr valley (approximately 10 km of railway tracks and 3000 sleeper tracks) as well as in the Rhine river valley (approximately 50 km of catenary and 220 km of tracks). The most damaged railway line (between the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "?response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# empyty CUDA cache\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     login(token=os.environ.get('HUGGINGFACE_TOKEN'))\n",
    "\n",
    "#     self.pipeline, self.tokenizer = self.initialize_model(model_name)\n",
    "\n",
    "# def initialize_model(self, model_name):\n",
    "#     # Tokenizer initialization\n",
    "#     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=200)\n",
    "# print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "# model = transformers.pipeline(model=\"google/gemma-3-4b-it\") # \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\") #\n",
    "# model(question=\"Where do I live?\", text_inputs=\"My name is Wolfgang and I live in Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "?AutoTokenizer.from_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## create workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_db()\n",
    "curs = conn.cursor()\n",
    "\n",
    "DOCS_DIR = \"../\" + s.settings.PATH_DATA + \"text_sources/\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(DOCS_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(f\"fetching: {filename}\")\n",
    "\n",
    "        file_path = os.path.join(DOCS_DIR, filename)\n",
    "        text = extract_text(file_path)\n",
    "        filename = Path(filename).stem\n",
    "        authors, title = authors, title = (\n",
    "            re.compile(r\"(.+?)[0-9]{4}(.*)?\").search(filename).groups()\n",
    "        )\n",
    "\n",
    "        entry = {\n",
    "            \"authors\": authors.strip(),\n",
    "            \"title\": title.strip(),\n",
    "            \"source\": \"dummy source\",\n",
    "            \"content\": text,\n",
    "            \"metadata\": {\n",
    "                \"tags\": [\"ahr_valley\", \"dummy_publication_type\"],\n",
    "                \"published_date\": re.findall(r\"[0-9]{4}\", filename)[0],\n",
    "            },\n",
    "        }\n",
    "    fill_db(TextSource(**entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test different embedding models\n",
    "\n",
    "# https://www.tigerdata.com/blog/finding-the-best-open-source-embedding-model-for-rag\n",
    "\n",
    "\n",
    "def create_vectorizer(embedding_model, embeddings_dimensions):\n",
    "    embeddings_view_name = (\n",
    "        # f\"{'essays'}{'_'}{embedding_model.replace('-','_')}{'_'}{'embeddings'}\"\n",
    "        f\"{embedding_model.replace('-', '_')}{'_content_embeddings'}\"\n",
    "    )\n",
    "\n",
    "    with connect_db() as conn:\n",
    "        with conn.cursor() as curs:\n",
    "            curs.execute(\n",
    "                f\"\"\"\n",
    "                SELECT ai.create_vectorizer(\n",
    "                    'text_source'::regclass,\n",
    "                    if_not_exists => true,\n",
    "                    loading => ai.loading_column('content'),\n",
    "                    embedding => ai.embedding_ollama('{embedding_model}', {embeddings_dimensions}),\n",
    "                    chunking => ai.chunking_recursive_character_text_splitter(\n",
    "                        {embeddings_dimensions}, {s.settings.CHUNK_OVERLAP}, \n",
    "                        separators => array[E'\\n\\n', E'\\n', '. ']\n",
    "                    ),\n",
    "                    destination =>  ai.destination_table(view_name => '{embeddings_view_name}'),\n",
    "                    formatting => ai.formatting_python_template('authors - title: $authors - $title, chunk: $chunk')\n",
    "                );\"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "# destination => {embeddings_view_name},  # Alternative to table: making just as a view\n",
    "#  ai.destination_table({embeddings_table_name})\n",
    "#  ai.chunking_character_text_splitter(128, 10, E'\\n'),\n",
    "#   embedding => ai.embedding_ollama({embedding_model}, {embeddings_dimensions}),\n",
    "# formating:  add the title of the document as the first line of the chunk\n",
    "\n",
    "EMBEDDING_MODELS = [\n",
    "    # {\"name\": \"all-minilm\", \"dimensions\": 384}\n",
    "    {\"name\": \"nomic-embed-text\", \"dimensions\": 768},\n",
    "    # {\"name\": \"mxbai-embed-large\", \"dimensions\": 1024},\n",
    "    # {\"name\": \"bge-m3\", \"dimensions\": 1024},\n",
    "]\n",
    "\n",
    "for model in EMBEDDING_MODELS:\n",
    "    create_vectorizer(model[\"name\"], model[\"dimensions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_db(\"SELECT * FROM ai.vectorizer_status;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all embedded docs from pgai postgres DB and apply LLM\n",
    "\n",
    "\n",
    "## # https://www.tigerdata.com/blog/finding-the-best-open-source-embedding-model-for-rag\n",
    "\n",
    "# def fetch_similar_chunks(question: str, top_k: int = 5):\n",
    "#     with connect_db() as conn:\n",
    "#         with conn.cursor() as curs:\n",
    "#             curs.execute(\n",
    "#                 f\"\"\"\n",
    "#                 SELECT content, ai.cosine_distance(\n",
    "#                     ai.embedding_ollama('nomic-embed-text', 768, %s),\n",
    "#                     embedding\n",
    "#                 ) AS distance\n",
    "#                 FROM nomic_embed_text_content_embeddings\n",
    "#                 ORDER BY distance ASC\n",
    "#                 LIMIT %s;\n",
    "#                 \"\"\",\n",
    "#             (question, top_k),\n",
    "#         )\n",
    "#         results = curs.fetchall()\n",
    "# return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the context text from the response\n",
    "context = \"\".join(context_response[\"context\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which societal or economic impacts of infrastructure failures are mentioned in the text?\"\n",
    "\n",
    "\n",
    "decoder_model = DecoderModel()\n",
    "response = decoder_model.generate_response(question=question, context=context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-impacts-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
